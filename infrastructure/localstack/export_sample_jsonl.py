import json
import os
import subprocess
from typing import Any

AWS_REGION = "ap-northeast-1"
MAX_ITEMS = 50  # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§ã‚¢ã‚¤ãƒ†ãƒ æ•°

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DESCRIBE_DIR = os.path.join(BASE_DIR, "dynamodb", "describe_tables")
OUTPUT_DIR = os.path.join(BASE_DIR, "dynamodb", "sample_data")

os.makedirs(OUTPUT_DIR, exist_ok=True)


def scan_table_sample(table_name: str, max_items: int) -> Any:
    print(f"ğŸ“¥ ãƒ†ãƒ¼ãƒ–ãƒ«ã€Œ{table_name}ã€ã‹ã‚‰æœ€å¤§ {max_items} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ (Scan)...")
    result = subprocess.run(
        [
            "aws",
            "dynamodb",
            "scan",
            "--table-name",
            table_name,
            "--region",
            AWS_REGION,
            "--limit",
            str(max_items),
        ],
        stdout=subprocess.PIPE,
        text=True,
        check=True,
    )
    return json.loads(result.stdout).get("Items", [])


def query_table_sample(table_name: str, pk_name: str, pk_value: str, max_items: int) -> Any:
    print(f"ğŸ“¥ ãƒ†ãƒ¼ãƒ–ãƒ«ã€Œ{table_name}ã€ã‹ã‚‰æœ€å¤§ {max_items} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­ (Query)...")
    result = subprocess.run(
        [
            "aws",
            "dynamodb",
            "query",
            "--table-name",
            table_name,
            "--region",
            AWS_REGION,
            "--limit",
            str(max_items),
            "--key-condition-expression",
            f"{pk_name} = :pkval",
            "--expression-attribute-values",
            json.dumps({":pkval": pk_value}),
        ],
        stdout=subprocess.PIPE,
        text=True,
        check=True,
    )
    return json.loads(result.stdout).get("Items", [])


def load_existing_jsonl(path: str) -> list[Any]:
    if not os.path.exists(path):
        return []
    with open(path) as f:
        return [json.loads(line) for line in f if line.strip()]


def merge_items(
    existing_items: Any, new_items: Any, pk_name: str, sk_name: str | None = None
) -> tuple[Any, int, int, int]:
    item_map = {}
    overwrite_count = 0
    new_count = 0
    unchanged_count = 0

    def make_key(item: Any) -> tuple[str, str | None]:
        pk_val = json.dumps(item[pk_name])
        sk_val = json.dumps(item[sk_name]) if sk_name and sk_name in item else None
        return (pk_val, sk_val)

    for item in existing_items:
        item_map[make_key(item)] = item

    for item in new_items:
        key = make_key(item)
        new_item_serialized = json.dumps(item, sort_keys=True)

        if key in item_map:
            existing_item_serialized = json.dumps(item_map[key], sort_keys=True)
            if new_item_serialized != existing_item_serialized:
                print(f"ğŸ” ä¸Šæ›¸ã: {pk_name}={key[0]}" + (f", {sk_name}={key[1]}" if sk_name else ""))
                item_map[key] = item
                overwrite_count += 1
            else:
                unchanged_count += 1
        else:
            print(f"â• æ–°è¦è¿½åŠ : {pk_name}={key[0]}" + (f", {sk_name}={key[1]}" if sk_name else ""))
            item_map[key] = item
            new_count += 1

    merged = list(item_map.values())
    return merged, new_count, overwrite_count, unchanged_count


# ãƒ¡ã‚¤ãƒ³å‡¦ç†
for filename in os.listdir(DESCRIBE_DIR):
    if not filename.endswith(".json"):
        continue

    table_name = filename.replace(".json", "")
    describe_path = os.path.join(DESCRIBE_DIR, filename)

    try:
        with open(describe_path) as f:
            desc = json.load(f)

        key_schema = desc["Table"]["KeySchema"]
        pk_name = key_schema[0]["AttributeName"]
        sk_name = key_schema[1]["AttributeName"] if len(key_schema) > 1 else None

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã« PK å…¥åŠ›ã‚’æ±‚ã‚ã‚‹
        pk_input = input(
            f"ğŸ”‘ ãƒ†ãƒ¼ãƒ–ãƒ«ã€Œ{table_name}ã€ã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚­ãƒ¼ã€Œ{pk_name}ã€ã®å€¤ã‚’æŒ‡å®šã—ã¾ã™ã‹ï¼Ÿ(ç©ºã§ scan): "
        ).strip()
        if pk_input:
            pk_value: Any = {desc["Table"]["AttributeDefinitions"][0]["AttributeType"]: pk_input}
            items = query_table_sample(table_name, pk_name, pk_value, MAX_ITEMS)
        else:
            items = scan_table_sample(table_name, MAX_ITEMS)

        if not items:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            continue

        output_path = os.path.join(OUTPUT_DIR, f"{table_name}.jsonl")
        existing_items = load_existing_jsonl(output_path)

        merged_items, new_count, overwrite_count, unchanged_count = merge_items(existing_items, items, pk_name, sk_name)

        with open(output_path, "w") as f:
            for item in merged_items:
                f.write(json.dumps(item) + "\n")

        print(
            f"âœ… ä¿å­˜ã—ã¾ã—ãŸã€‚ğŸ§¾ ç™»éŒ²æ¸ˆã¿: {len(merged_items)} ä»¶ "
            f"ï¼ˆæ–°è¦ {new_count} ä»¶ã€ä¸Šæ›¸ã {overwrite_count} ä»¶ã€æœªæ›´æ–° {unchanged_count} ä»¶ï¼‰\n"
        )

    except subprocess.CalledProcessError as e:
        print(f"âŒ ãƒ†ãƒ¼ãƒ–ãƒ«ã€Œ{table_name}ã€ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    except Exception as e:
        print(f"âŒ æƒ³å®šå¤–ã®ã‚¨ãƒ©ãƒ¼: {e}")
